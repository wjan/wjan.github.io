In this article I'll walk you through my first experiment related to environment object mapping using object detection and mono depth ML algorithms. Inspired with Tesla autopilot and Full Self Driving suites surroundings real time 3D visualization I'll try to recreate similar. 

To simplyfi the process I'll work with a camera that is placed in my living room.

For the sake of this experiment I'll use two prebaked PyTorch models, both available on PyTorch hub:
* Yolo v5: https://pytorch.org/hub/ultralytics_yolov5/ - responsible for handling basic object detection, pretrained model can recognize multiple and general known object classes.
* MiDaS: https://pytorch.org/hub/intelisl_midas_v2/ - responsible for monocular dept estimation, in oder words this ML model can estimate the distance from the camera to the object. The beauty of this model is that it works quite well despite of the fact it uses only single image (thus 2D vision instead of 3D vision).

In this experiment I'll try to combine both ML model in order to create 2D spacial map of objects found in an image or video. Some custom Python code will be written to tie the process together and visualize the 2D map of objects.

The high level algorithm will consist of following steps:
1. Retrieve an image (i.e. a single video frame)
2. Process the image with monodepth MiDaS model. It will return a heat map with an estimation of depth for every pixel.
3. Process the image with Yolo object detection model. It will return a list of found objects with their coordinates.
4. Match every object found using Yolo with a heatmap.  Object coordinates will be used to get the MiDaS depth.
5. Show the image on a screen using 2D visualization graphics library.

## Screenshots
Algorithm input (Step 2) returns of the algorithm returns a heatmap that could be visualized as follows:
![MiDaS](https://raw.githubusercontent.com/wjan/wjan.github.io/main/img/midas.png)

Another algorithm input (Step 3) returns an array of detected objects that could be visualized as follows:
![Yolo](https://raw.githubusercontent.com/wjan/wjan.github.io/main/img/yolo.png)

The output generated by the algorithmm (steps 4 and 5 combined) gives a 2D map of the photographed area:
![Yolo + MiDaS](https://raw.githubusercontent.com/wjan/wjan.github.io/main/img/molo.png)



Conclusions:
1. Due to the nature of self driving environment solution for this problem might have some possible simplifications but also complication i.e.:
* Self driving simplification: car drivers have to navigate on a 2D plane instead of 3D space. Thus problem with height and Z axis position estimation rarely occurs. In this POC we have to deal with an environment with Z axis, let's say there is a glass on the top of the table and those object have to be correctly mapped one on top of another. Such positioning rarely occurs for the objects relevant for driving - cars, pedestrians and obstacles are always placed on top of one and only surface.
* Self driving comlication: the object from where the mapping takes place (which is basically a set of camera) is either stopped or moving. Most of the times the camera will be moving with the car so the mapped environment will be changing constantly and those environment changes should be presented gracefully with some animation. Solving the animation will most probably involve maintaning a memory of recognized objects, so mechanism will keep track of unique objects around.
3. Camera image quality, lightning and recognized object flickering (possibly due to image compression) were challenging to provide a good quality of 2D spatial map. I.e. coordinates of frames for recognized objects may vary depending on a frame. The location of frame in time might be flickering (or jumping) so objects may seem as they are chaning position even when not moving. 

Potential direction to improve the experiment from here:
1. Room wall gemoetry detection and adjustment - in this experiment camera was placed in the corner of the room, so the 2D map doesn't reflect the room geometry correctly. To solve that the algorithm could be enhanced by wall (boundary) detection that could be used to further correct the 2D map item placement.
2. 3D space visualization - so far the algorithm doesn't detect the Z axis position of the detected object. Although self driving object mapping doesn't require Z axis to work it would be nice to have it for room mapping.
3. Object size visualization - this POC experiment doesn't output the size of the detected image.
